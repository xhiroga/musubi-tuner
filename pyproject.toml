[project]
name = "musubi-tuner"
version = "0.1.0"
description = "Musubi Tuner by kohya_ss"
readme = "README.md"
requires-python = ">=3.10, <3.11"
dependencies = [
    "accelerate>=1.6.0",
    "ascii-magic==2.3.0",
    "av==14.0.1",
    "bitsandbytes>=0.45.0",
    "diffusers>=0.32.1",
    "easydict==1.13",
    "einops>=0.7.0",
    "flash-attn",
    "ftfy==6.3.1",
    "huggingface-hub>=0.30.0",
    "matplotlib>=3.10.0",
    "opencv-python>=4.10.0.84",
    "pillow>=10.2.0",
    "pytorch-triton>=3.3.0",
    "safetensors>=0.4.5",
    "sageattention>=1.0.6",
    "schedulefree>=1.4.1",
    "tensorboard>=2.18.0",
    "toml>=0.10.2",
    "torch>=2.5.1",
    "torchvision>=0.20.1",
    "tqdm>=4.66.5",
    "transformers>=4.46.3",
    "voluptuous>=0.15.2",
    "wandb>=0.19.11",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.hooks.custom]
path = "hatch_build.py"

[[tool.uv.index]]
name = "pytorch-cu124"
url = "https://download.pytorch.org/whl/cu124"
explicit = true

[tool.uv.sources]
pytorch-triton = { index = "pytorch-cu124" }
torch = { index = "pytorch-cu124" }
torchvision = { index = "pytorch-cu124" }
flash-attn = { url = "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3+cu124torch2.5-cp310-cp310-linux_x86_64.whl" }
