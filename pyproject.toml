[project]
name = "musubi-tuner"
version = "0.1.0"
description = "Musubi Tuner by kohya_ss"
readme = "README.md"
requires-python = ">=3.11, <3.12"
dependencies = [
    "accelerate>=1.6.0",
    "ascii-magic==2.3.0",
    "av==14.0.1",
    "bitsandbytes>=0.45.0",
    "diffusers>=0.32.1",
    "easydict==1.13",
    "einops>=0.7.0",
    "flash-attn",
    "ftfy==6.3.1",
    "huggingface-hub>=0.30.0",
    "matplotlib>=3.10.0",
    "opencv-python>=4.10.0.84",
    "pillow>=10.2.0",
    "pytorch-triton>=3.3.0",
    "safetensors>=0.4.5",
    "sageattention>=1.0.6",
    "schedulefree>=1.4.1",
    "tensorboard>=2.18.0",
    "toml>=0.10.2",
    "torch>=2.8.0.dev20250525",
    "torchvision>=0.22.0.dev20250525",
    "tqdm>=4.66.5",
    "transformers>=4.46.3",
    "voluptuous>=0.15.2",
    "wandb>=0.19.11",
]

[[tool.uv.index]]
name = "pytorch-nightly-cu128"
url = "https://download.pytorch.org/whl/nightly/cu128"
explicit = true

[tool.uv.sources]
pytorch-triton = { index = "pytorch-nightly-cu128" }
torch = { index = "pytorch-nightly-cu128" }
torchvision = { index = "pytorch-nightly-cu128" }
flash-attn = { url = "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.2.1/flash_attn-2.4.3+cu128torch2.8-cp311-cp311-linux_x86_64.whl" }
